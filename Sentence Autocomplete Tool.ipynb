{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NiMPag8IGcG",
        "outputId": "ec55be31-f0fe-42c9-8d9b-7e21722cf86b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-20 19:26:40--  http://wstyler.ucsd.edu/files/enronsentv1.tar.gz\n",
            "Resolving wstyler.ucsd.edu (wstyler.ucsd.edu)... 132.239.147.75\n",
            "Connecting to wstyler.ucsd.edu (wstyler.ucsd.edu)|132.239.147.75|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://wstyler.ucsd.edu/files/enronsentv1.tar.gz [following]\n",
            "--2023-05-20 19:26:41--  https://wstyler.ucsd.edu/files/enronsentv1.tar.gz\n",
            "Connecting to wstyler.ucsd.edu (wstyler.ucsd.edu)|132.239.147.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26406491 (25M) [application/x-gzip]\n",
            "Saving to: ‘enronsentv1.tar.gz’\n",
            "\n",
            "enronsentv1.tar.gz  100%[===================>]  25.18M  24.3MB/s    in 1.0s    \n",
            "\n",
            "2023-05-20 19:26:42 (24.3 MB/s) - ‘enronsentv1.tar.gz’ saved [26406491/26406491]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tarfile\n",
        "!wget http://wstyler.ucsd.edu/files/enronsentv1.tar.gz\n",
        "my_tar = tarfile.open('/content/enronsentv1.tar.gz')\n",
        "my_tar.extractall('/content/') # specify which folder to extract to\n",
        "my_tar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tqanPOC64Z1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "data_dir = '/content/enronsent'\n",
        "train_files = 10\n",
        "val_files = 5\n",
        "train_text = \"\"\n",
        "val_text = \"\"\n",
        "\n",
        "for i in range(train_files):\n",
        "    filename = os.path.join(data_dir, f'enronsent{i:02d}')\n",
        "    with open(filename, 'r') as file:\n",
        "        content = file.read()\n",
        "        train_text += content\n",
        "\n",
        "for i in range(train_files, train_files + val_files):\n",
        "    filename = os.path.join(data_dir, f'enronsent{i:02d}')\n",
        "    with open(filename, 'r') as file:\n",
        "        content = file.read()\n",
        "        val_text += content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEkgdIu-ZEpV",
        "outputId": "287c5e1a-961b-4684-dd29-0dd25daaa165"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPsFGjRAbXzh"
      },
      "outputs": [],
      "source": [
        "#preprocessing the data (remove punctuation, spaces, digits and stopwords / make everything to lower case / stemming and lemmtization)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocessingAndgettokens(text):\n",
        "    cleaned_data = []\n",
        "    for j in text:\n",
        "        j = j.lower()\n",
        "        j = j.translate(str.maketrans('', '', string.punctuation))\n",
        "        j = \" \".join(j.split())\n",
        "        j = ''.join(c for c in j if not c.isdigit())\n",
        "        word_tokens = word_tokenize(j)\n",
        "        stop_words = set(stopwords.words(\"english\"))\n",
        "        filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "        cleaned_data.append(filtered_text)\n",
        "    return cleaned_data\n",
        "\n",
        "train_data = preprocessingAndgettokens([train_text])[0]\n",
        "val_data = preprocessingAndgettokens([val_text])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUj2_9v4IGcJ"
      },
      "outputs": [],
      "source": [
        "def create_dataset(words, window_size, stride):\n",
        "    X, y = [], []\n",
        "    for i in range(0, len(words) - window_size + 1, stride):\n",
        "        window = words[i:i+window_size]\n",
        "        X.append(window[:-1])\n",
        "        y.append(window[-1])\n",
        "    return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxRKb_JlIGcJ"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, = create_dataset(train_data, window_size=10, stride=10)\n",
        "X_val, y_val =  create_dataset(val_data, window_size=10, stride=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnEWdGFajFGc"
      },
      "outputs": [],
      "source": [
        "del train_data, val_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PHpVt-pIGcJ",
        "outputId": "94b23b92-8b54-447c-bb4c-73b428a6e45e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(181763, 9) (181763,) (84542, 9) (84542,)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuLLaQkv5jBd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import gensim.downloader as api\n",
        "from tqdm import tqdm\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "pretreained_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "max_sequence_length = 10\n",
        "embedding_dim = pretreained_model.vector_size\n",
        "embedding_matrix = np.zeros((len(pretreained_model.key_to_index) + 1, embedding_dim))\n",
        "for word, index in pretreained_model.key_to_index.items():\n",
        "    embedding_matrix[index + 1] = pretreained_model.get_vector(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUjb8PnoY39c",
        "outputId": "f3a860be-2469-4823-abc7-c7ddeed739a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 266305/266305 [00:00<00:00, 467881.10it/s]\n"
          ]
        }
      ],
      "source": [
        "corpus = (np.concatenate((X_train, X_val))).tolist()\n",
        "trained_model = Word2Vec(sentences=tqdm(corpus), vector_size=300, window=10, min_count=1, workers=4, epochs=50)\n",
        "\n",
        "max_sequence_length = 10\n",
        "embedding_dim = trained_model.vector_size\n",
        "embedding_matrix = np.zeros((len(trained_model.wv.key_to_index) + 1, embedding_dim))\n",
        "for word, index in trained_model.wv.key_to_index.items():\n",
        "    embedding_matrix[index + 1] = trained_model.wv[word]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWrHmpCN8FfF"
      },
      "outputs": [],
      "source": [
        "def preprocess_sequences(sequences, model, max_sequence_length):\n",
        "    sequences_encoded = [[model.wv.key_to_index.get(word, 0) for word in sublist] for sublist in sequences]\n",
        "    sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(sequences_encoded, maxlen=max_sequence_length)\n",
        "    return sequences_padded\n",
        "\n",
        "\n",
        "\n",
        "def encode_targets(target_words):\n",
        "    word_to_int = {word: i for i, word in enumerate(set(target_words))}\n",
        "    num_classes = len(word_to_int)\n",
        "    target_encoded = [word_to_int[word] for word in target_words]\n",
        "    target_encoded = np.array(target_encoded)\n",
        "\n",
        "    return target_encoded, word_to_int, num_classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWorW2H28fOv"
      },
      "outputs": [],
      "source": [
        "y_train_encoded, word_to_int_train, num_classes_train = encode_targets(y_train)\n",
        "y_val_encoded, word_to_int_val, num_classes_val = encode_targets(y_val)\n",
        "X_train_padded = preprocess_sequences(X_train, trained_model, max_sequence_length)\n",
        "X_val_padded = preprocess_sequences(X_val, trained_model, max_sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPjaTx7djmtH"
      },
      "outputs": [],
      "source": [
        "del X_train, X_val, y_train, y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5wNOz6s8y25"
      },
      "outputs": [],
      "source": [
        "embedding_layer = Embedding(\n",
        "    input_dim=len(trained_model.wv.key_to_index) + 1,\n",
        "    output_dim=embedding_dim,\n",
        "    input_length=max_sequence_length,\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=False\n",
        ")\n",
        "\n",
        "AutoComplete = Sequential()\n",
        "AutoComplete.add(embedding_layer)\n",
        "AutoComplete.add(LSTM(units=256, return_sequences=True))\n",
        "AutoComplete.add(LSTM(units=256))\n",
        "AutoComplete.add(Dense(units=num_classes_train, activation='softmax'))\n",
        "AutoComplete.add(Dropout(0.2))\n",
        "\n",
        "AutoComplete.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHeKJZ3EEhS6",
        "outputId": "e9924110-7ca2-4f9d-f54c-5fb2eb3af878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "2841/2841 [==============================] - 46s 13ms/step - loss: 9.7791 - accuracy: 0.0200 - val_loss: 11.2927 - val_accuracy: 1.1828e-05\n",
            "Epoch 2/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 9.3968 - accuracy: 0.0384 - val_loss: 11.7250 - val_accuracy: 1.1828e-05\n",
            "Epoch 3/50\n",
            "2841/2841 [==============================] - 41s 14ms/step - loss: 9.0460 - accuracy: 0.0523 - val_loss: 12.3402 - val_accuracy: 2.3657e-05\n",
            "Epoch 4/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 8.6168 - accuracy: 0.0689 - val_loss: 12.8828 - val_accuracy: 2.3657e-05\n",
            "Epoch 5/50\n",
            "2841/2841 [==============================] - 37s 13ms/step - loss: 8.0715 - accuracy: 0.0959 - val_loss: 13.7881 - val_accuracy: 1.1828e-05\n",
            "Epoch 6/50\n",
            "2841/2841 [==============================] - 42s 15ms/step - loss: 7.5095 - accuracy: 0.1447 - val_loss: 14.2263 - val_accuracy: 5.9142e-05\n",
            "Epoch 7/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 6.9829 - accuracy: 0.1973 - val_loss: 14.5748 - val_accuracy: 7.0971e-05\n",
            "Epoch 8/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 6.5367 - accuracy: 0.2439 - val_loss: 14.8070 - val_accuracy: 1.1828e-05\n",
            "Epoch 9/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 6.1502 - accuracy: 0.2864 - val_loss: 14.9418 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/50\n",
            "2841/2841 [==============================] - 37s 13ms/step - loss: 5.8522 - accuracy: 0.3228 - val_loss: 15.1327 - val_accuracy: 3.5485e-05\n",
            "Epoch 11/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 5.5698 - accuracy: 0.3544 - val_loss: 15.1912 - val_accuracy: 4.7314e-05\n",
            "Epoch 12/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 5.3244 - accuracy: 0.3865 - val_loss: 15.2767 - val_accuracy: 4.7314e-05\n",
            "Epoch 13/50\n",
            "2841/2841 [==============================] - 37s 13ms/step - loss: 5.0930 - accuracy: 0.4143 - val_loss: 15.3257 - val_accuracy: 9.4628e-05\n",
            "Epoch 14/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 4.9179 - accuracy: 0.4385 - val_loss: 15.4042 - val_accuracy: 4.7314e-05\n",
            "Epoch 15/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 4.7606 - accuracy: 0.4607 - val_loss: 15.4606 - val_accuracy: 5.9142e-05\n",
            "Epoch 16/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 4.6175 - accuracy: 0.4801 - val_loss: 15.5085 - val_accuracy: 2.3657e-05\n",
            "Epoch 17/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 4.4920 - accuracy: 0.4980 - val_loss: 15.5498 - val_accuracy: 7.0971e-05\n",
            "Epoch 18/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 4.3418 - accuracy: 0.5148 - val_loss: 15.5861 - val_accuracy: 3.5485e-05\n",
            "Epoch 19/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 4.2504 - accuracy: 0.5288 - val_loss: 15.6125 - val_accuracy: 2.3657e-05\n",
            "Epoch 20/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 4.1429 - accuracy: 0.5414 - val_loss: 15.6547 - val_accuracy: 9.4628e-05\n",
            "Epoch 21/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 4.0438 - accuracy: 0.5543 - val_loss: 15.6726 - val_accuracy: 4.7314e-05\n",
            "Epoch 22/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 3.9840 - accuracy: 0.5629 - val_loss: 15.6998 - val_accuracy: 4.7314e-05\n",
            "Epoch 23/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 3.9131 - accuracy: 0.5717 - val_loss: 15.7219 - val_accuracy: 7.0971e-05\n",
            "Epoch 24/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 3.8555 - accuracy: 0.5799 - val_loss: 15.7494 - val_accuracy: 4.7314e-05\n",
            "Epoch 25/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 3.7987 - accuracy: 0.5879 - val_loss: 15.7715 - val_accuracy: 7.0971e-05\n",
            "Epoch 26/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 3.7464 - accuracy: 0.5919 - val_loss: 15.7784 - val_accuracy: 3.5485e-05\n",
            "Epoch 27/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 3.7235 - accuracy: 0.5971 - val_loss: 15.7856 - val_accuracy: 4.7314e-05\n",
            "Epoch 28/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 3.6575 - accuracy: 0.6047 - val_loss: 15.8056 - val_accuracy: 2.3657e-05\n",
            "Epoch 29/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 3.6167 - accuracy: 0.6086 - val_loss: 15.8224 - val_accuracy: 4.7314e-05\n",
            "Epoch 30/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 3.5942 - accuracy: 0.6109 - val_loss: 15.8327 - val_accuracy: 3.5485e-05\n",
            "Epoch 31/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 3.5713 - accuracy: 0.6150 - val_loss: 15.8415 - val_accuracy: 2.3657e-05\n",
            "Epoch 32/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 3.5422 - accuracy: 0.6180 - val_loss: 15.8542 - val_accuracy: 2.3657e-05\n",
            "Epoch 33/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 3.5175 - accuracy: 0.6205 - val_loss: 15.8611 - val_accuracy: 3.5485e-05\n",
            "Epoch 34/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 3.4798 - accuracy: 0.6243 - val_loss: 15.8765 - val_accuracy: 5.9142e-05\n",
            "Epoch 35/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 3.4485 - accuracy: 0.6277 - val_loss: 15.8828 - val_accuracy: 8.2799e-05\n",
            "Epoch 36/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 3.4517 - accuracy: 0.6275 - val_loss: 15.8945 - val_accuracy: 7.0971e-05\n",
            "Epoch 37/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 3.4357 - accuracy: 0.6292 - val_loss: 15.8974 - val_accuracy: 4.7314e-05\n",
            "Epoch 38/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 3.3887 - accuracy: 0.6337 - val_loss: 15.9000 - val_accuracy: 5.9142e-05\n",
            "Epoch 39/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 3.4043 - accuracy: 0.6321 - val_loss: 15.9111 - val_accuracy: 1.0646e-04\n",
            "Epoch 40/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 3.3786 - accuracy: 0.6328 - val_loss: 15.9197 - val_accuracy: 1.1828e-05\n",
            "Epoch 41/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 3.3649 - accuracy: 0.6366 - val_loss: 15.9292 - val_accuracy: 7.0971e-05\n",
            "Epoch 42/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 3.3594 - accuracy: 0.6363 - val_loss: 15.9280 - val_accuracy: 1.1828e-05\n",
            "Epoch 43/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 3.3262 - accuracy: 0.6385 - val_loss: 15.9334 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 3.3423 - accuracy: 0.6368 - val_loss: 15.9382 - val_accuracy: 8.2799e-05\n",
            "Epoch 45/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 3.3336 - accuracy: 0.6373 - val_loss: 15.9436 - val_accuracy: 4.7314e-05\n",
            "Epoch 46/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 3.3176 - accuracy: 0.6393 - val_loss: 15.9507 - val_accuracy: 3.5485e-05\n",
            "Epoch 47/50\n",
            "2841/2841 [==============================] - 39s 14ms/step - loss: 3.3125 - accuracy: 0.6413 - val_loss: 15.9534 - val_accuracy: 4.7314e-05\n",
            "Epoch 48/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 3.2981 - accuracy: 0.6427 - val_loss: 15.9583 - val_accuracy: 4.7314e-05\n",
            "Epoch 49/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 3.2825 - accuracy: 0.6433 - val_loss: 15.9610 - val_accuracy: 4.7314e-05\n",
            "Epoch 50/50\n",
            "2841/2841 [==============================] - 36s 13ms/step - loss: 3.3062 - accuracy: 0.6413 - val_loss: 15.9655 - val_accuracy: 4.7314e-05\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcb0d4b9e40>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "AutoComplete.fit(X_train_padded, y_train_encoded, validation_data=(X_val_padded, y_val_encoded), epochs=50, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_input_sequence(sequence, model, max_sequence_length):\n",
        "    word_to_index = {word: index for index, word in enumerate(model.wv.index_to_key)}\n",
        "    sequence_encoded = [word_to_index.get(word, 0) for word in sequence]\n",
        "    sequence_padded = tf.keras.preprocessing.sequence.pad_sequences([sequence_encoded], maxlen=max_sequence_length)\n",
        "    return sequence_padded"
      ],
      "metadata": {
        "id": "7IoYxu_ubNJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int_to_word_train = {value: key for key, value in word_to_int_train.items()}\n",
        "int_to_word_val = {value: key for key, value in word_to_int_val.items()}\n",
        "\n",
        "int_to_word_combined = {}\n",
        "int_to_word_combined.update(int_to_word_train)\n",
        "int_to_word_combined.update(int_to_word_val)"
      ],
      "metadata": {
        "id": "l0kyKwVgg6j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHv75LwWdVnw"
      },
      "outputs": [],
      "source": [
        "def SentenceAutoComplete(pretreained_model, max_sequence_length, int_to_word_combined):\n",
        "    predicted_sequence = []  # Sequence of correct predictions and user inputs\n",
        "\n",
        "    while True:\n",
        "        word = input(\"Enter Next word (-1 to terminate): \")\n",
        "        if word == \"-1\":\n",
        "            break\n",
        "\n",
        "        predicted_sequence.append(word)\n",
        "        input_sequence = preprocess_input_sequence(predicted_sequence, pretreained_model, max_sequence_length)\n",
        "        predicted_word = AutoComplete.predict(input_sequence)\n",
        "        predicted_word = int_to_word_combined[np.argmax(predicted_word)]\n",
        "        print(f\"Is your next word: '{predicted_word}'\")\n",
        "\n",
        "        while True:\n",
        "            user_input = input(\"Yes/No: \")\n",
        "            if user_input.lower() == \"no\":\n",
        "                break\n",
        "            elif user_input.lower() == 'yes':\n",
        "              predicted_sequence.append(predicted_word)\n",
        "              input_sequence = preprocess_input_sequence(predicted_sequence, pretreained_model, max_sequence_length)\n",
        "              predicted_word = AutoComplete.predict(input_sequence)\n",
        "              predicted_word = int_to_word_combined[np.argmax(predicted_word)]\n",
        "              print(f\"Is your next word: '{predicted_word}'\")\n",
        "            else:\n",
        "              print(\"wrong Answer\")\n",
        "              continue\n",
        "\n",
        "    print(\"Your final Sentence is:\", ' '.join(predicted_sequence))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SentenceAutoComplete(trained_model, max_sequence_length, int_to_word_combined)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfGdG7k5SPIx",
        "outputId": "a8b63854-c4b4-4a50-9bd0-6c9d08c4175e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Next word (-1 to terminate): hello\n",
            "1/1 [==============================] - 1s 770ms/step\n",
            "Is your next word: 'yere'\n",
            "Yes/No: no\n",
            "Enter Next word (-1 to terminate): my\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Is your next word: 'good'\n",
            "Yes/No: no\n",
            "Enter Next word (-1 to terminate): name\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Is your next word: 'forcing'\n",
            "Yes/No: no\n",
            "Enter Next word (-1 to terminate): is\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Is your next word: 'parameters'\n",
            "Yes/No: yes\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Is your next word: 'appl'\n",
            "Yes/No: no\n",
            "Enter Next word (-1 to terminate): -1\n",
            "Your final Sentence is: hello my name is parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SentenceAutoComplete(trained_model, max_sequence_length, int_to_word_combined)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6mfbDczqeXH",
        "outputId": "96065fdd-5fe6-4d46-e7ab-370378dad632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Next word (-1 to terminate): the\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Is your next word: 'ogc'\n",
            "Yes/No: no\n",
            "Enter Next word (-1 to terminate): world\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Is your next word: 'chicfila'\n",
            "Yes/No: no\n",
            "Enter Next word (-1 to terminate): is\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Is your next word: 'suffered'\n",
            "Yes/No: no\n",
            "Enter Next word (-1 to terminate): -1\n",
            "Your final Sentence is: the world is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SentenceAutoComplete(trained_model, max_sequence_length, int_to_word_combined)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUdr3lD9qetA",
        "outputId": "60488c74-30b7-4c22-d02a-6c28fc83cb77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Next word (-1 to terminate): hey\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Is your next word: 'dc'\n",
            "Yes/No: no\n",
            "Enter Next word (-1 to terminate): there\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Is your next word: 'suffered'\n",
            "Yes/No: no\n",
            "Enter Next word (-1 to terminate): mate\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Is your next word: 'doc'\n",
            "Yes/No: no\n",
            "Enter Next word (-1 to terminate): -1\n",
            "Your final Sentence is: hey there mate\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}